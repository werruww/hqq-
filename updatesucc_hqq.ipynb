{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypBqLLeWAILV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lHJbpf5DAK21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YfBkfe1jAK57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "ha86PwcMAK8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hqq"
      ],
      "metadata": {
        "id": "lpFPecDOAK-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "xujaJkNEBMc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH)"
      ],
      "metadata": {
        "id": "VtWKnhqHCuHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=100, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "dmCXNYzmDHHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6gs1eUyYHLC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mIX-rjgNHLAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8FFov-sHK8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "l1ewE8nDHLhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "#prepare_for_inference(model)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH_COMPILE)"
      ],
      "metadata": {
        "id": "gxjTcfgzH6QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AD1DPUsCHgzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "gen.generate(\"What is the result of the following addition operation 34+67?\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "Yi44oXBzHiH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitblas"
      ],
      "metadata": {
        "id": "gav_4HhlLlcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "patch_linearlayers(model, patch_add_quant_config,\n",
        "                          BaseQuantizeConfig(nbits=2, group_size=64, quant_scale=False, quant_zero=False, axis=1))\n",
        "\n",
        "model.eval();\n",
        "cleanup()\n",
        "\n",
        "#Use optimized inference kernels\n",
        "###################################################\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "#prepare_for_inference(model) #default backend\n",
        "prepare_for_inference(model, backend=\"bitblas\", allow_merge=False) #It takes a while...\n",
        "\n",
        "#Generate\n",
        "###################################################\n",
        "#For longer context, make sure to allocate enough cache via the cache_size= parameter\n",
        "#gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=None) #Slower generation but no warm-up\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "9f8hkChXLhUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11pH7yv_PhXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "\n",
        "#Prepare the model for inference\n",
        "#HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "#prepare_for_inference(model) #default backend\n",
        "\n",
        "# Use a try-except block to handle the TypeError\n",
        "try:\n",
        "    prepare_for_inference(model, backend=\"bitblas\", allow_merge=False) #It takes a while...\n",
        "except TypeError:\n",
        "    # If TypeError occurs, fallback to the PyTorch backend\n",
        "    print(\"Falling back to PyTorch backend due to TypeError in prepare_for_inference.\")\n",
        "    HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "    prepare_for_inference(model)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "z65cC-vnPiuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install hqq\n",
        "!pip install bitblas"
      ],
      "metadata": {
        "id": "ozg_89NhP0f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "patch_linearlayers(model, patch_add_quant_config,\n",
        "                          BaseQuantizeConfig(nbits=2, group_size=64, quant_scale=False, quant_zero=False, axis=1))\n",
        "\n",
        "model.eval()\n",
        "cleanup()\n",
        "\n",
        "#Use optimized inference kernels\n",
        "###################################################\n",
        "# Try using bitblas backend, fallback to PyTorch if TypeError occurs\n",
        "try:\n",
        "    from hqq.utils.patching import prepare_for_inference\n",
        "    HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "    prepare_for_inference(model, backend=\"bitblas\", allow_merge=False) #It takes a while...\n",
        "except TypeError:\n",
        "    print(\"Falling back to PyTorch backend due to TypeError in prepare_for_inference.\")\n",
        "    HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "    prepare_for_inference(model)\n",
        "\n",
        "#Generate\n",
        "###################################################\n",
        "#For longer context, make sure to allocate enough cache via the cache_size= parameter\n",
        "#gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=None) #Slower generation but no warm-up\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ITWAyclTPxcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float32, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "patch_linearlayers(model, patch_add_quant_config,\n",
        "                          BaseQuantizeConfig(nbits=2, group_size=64, quant_scale=False, quant_zero=False, axis=1))\n",
        "\n",
        "model.eval()\n",
        "cleanup()\n",
        "\n",
        "#Use optimized inference kernels\n",
        "###################################################\n",
        "# Try using bitblas backend, fallback to PyTorch if TypeError occurs\n",
        "try:\n",
        "    from hqq.utils.patching import prepare_for_inference\n",
        "    HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "    prepare_for_inference(model, backend=\"bitblas\", allow_merge=False) #It takes a while...\n",
        "except TypeError:\n",
        "    print(\"Falling back to PyTorch backend due to TypeError in prepare_for_inference.\")\n",
        "    HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "    prepare_for_inference(model)\n",
        "\n",
        "#Generate\n",
        "###################################################\n",
        "#For longer context, make sure to allocate enough cache via the cache_size= parameter\n",
        "#gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=None) #Slower generation but no warm-up\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)"
      ],
      "metadata": {
        "id": "EadoA4i-TPIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_for_inference(model)\n"
      ],
      "metadata": {
        "id": "s8vaCFXEV-j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n"
      ],
      "metadata": {
        "id": "EJiu84fSWyxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)\n"
      ],
      "metadata": {
        "id": "EAB73pEIWy3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True, compile=\"partial\")\n"
      ],
      "metadata": {
        "id": "S_RZ4IJUWy6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gen)\n",
        "print(type(gen))\n"
      ],
      "metadata": {
        "id": "RThNWUO3XMGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtype_ = fp_param[0].dtype if (len(fp_param) > 0) else patch_param[\"dtype\"]\n"
      ],
      "metadata": {
        "id": "EmsLbCRUXS62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "print(type(model))\n"
      ],
      "metadata": {
        "id": "1Un_iMy6XYSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp_param = [p for p in model.parameters() if p.is_floating_point()]\n",
        "print(fp_param)\n"
      ],
      "metadata": {
        "id": "jwP7KE0GXdAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_linearlayers(model, patch_add_quant_config,\n",
        "                   BaseQuantizeConfig(nbits=2, group_size=64, quant_scale=False,\n",
        "                                      quant_zero=False, axis=1, dtype=torch.float32))\n"
      ],
      "metadata": {
        "id": "gB69DRZfXwj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_for_inference(model)"
      ],
      "metadata": {
        "id": "A0AiZ3aVX3t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "prepare_for_inference(model)\n"
      ],
      "metadata": {
        "id": "DdAKESF0Xwm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp_param = [p for p in model.parameters() if p.is_floating_point()]\n",
        "print(f\"Floating-point parameters found: {len(fp_param)}\")\n"
      ],
      "metadata": {
        "id": "szyQP2PeYA3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True, compile=\"partial\").warmup()\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "RB5ix6LEXwpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "prepare_for_inference(model)\n"
      ],
      "metadata": {
        "id": "bbMXQ9gKXwsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "model.eval()  # Ensure it's in evaluation mode\n"
      ],
      "metadata": {
        "id": "rIHcfMekYgko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=1, do_sample=True, compile=None)\n",
        "output = gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "7kRIH7pNYuco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True, compile=None)\n",
        "output = gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "DCsJuJIwZIIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=22, do_sample=True, compile=None)\n",
        "output = gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "m5jVsYsNZMt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=22, do_sample=True, compile=None)\n",
        "output = gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "EbERIepPZMxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJqE4BYcZMzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "# Load the model\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model = AutoHQQHFModel.from_quantized(\n",
        "    model_id, cache_dir='.', compute_dtype=torch.float32, adapter='adapter_v0.1.lora'\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "patch_linearlayers(\n",
        "    model,\n",
        "    patch_add_quant_config,\n",
        "    BaseQuantizeConfig(nbits=2, group_size=64, quant_scale=False, quant_zero=False, axis=1),\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "cleanup()\n",
        "\n",
        "# Use optimized inference kernels (with fallback to PyTorch)\n",
        "try:\n",
        "    HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "    prepare_for_inference(model, backend=\"bitblas\", allow_merge=False)  # It takes a while...\n",
        "except TypeError as e:\n",
        "    print(\"Falling back to PyTorch backend due to TypeError in prepare_for_inference:\", e)\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print the full traceback\n",
        "    print(\"Model state:\", model) # Check if the model object itself is None or has unexpected attributes\n",
        "    # Add more print statements to inspect potentially problematic variables\n",
        "    HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "    prepare_for_inference(model)\n",
        "\n",
        "# Generate\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True, compile=\"partial\")\n",
        "# Try to catch potential errors during warmup:\n",
        "try:\n",
        "    gen.warmup()  # Faster generation, but warm-up takes a while\n",
        "except TypeError as e:\n",
        "    print(\"Error during warmup:\", e)\n",
        "    import traceback\n",
        "    traceback.print_exc() # Print the full traceback\n",
        "    print(\"Generator state:\", gen) # Check if the generator object has unexpected attributes\n",
        "    # Add more print statements to inspect potentially problematic variables\n",
        "\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GDZDRmJ2WBN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7vF0hXOVVjwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@pies052022/typeerror-nonetype-object-is-not-subscriptable-solved-6658a2ec69c3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "https://itsourcecode.com/typeerror/typeerror-nonetype-object-is-not-subscriptable/?source=post_page-----6658a2ec69c3--------------------------------"
      ],
      "metadata": {
        "id": "z6ZF4JG_UZfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = None\n",
        "print(my_list[0])"
      ],
      "metadata": {
        "id": "Pg96fjPzQULH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "s_list = None\n",
        "if s_list is not None:\n",
        "    print(s_list[0])\n",
        "else:\n",
        "    print(\"The sample list is None!\")"
      ],
      "metadata": {
        "id": "lFzvlRHTUetw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "print(s_list[0])"
      ],
      "metadata": {
        "id": "OCTMNwhTQUIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchao"
      ],
      "metadata": {
        "id": "KrYsrBQyQUFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUCHNNHIQUCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm"
      ],
      "metadata": {
        "id": "__JnyRcyQT_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM\n",
        "from vllm import SamplingParams\n",
        "import random\n",
        "\n",
        "# Initialize the language model\n",
        "llm = LLM(model=\"/content/models--mobiuslabsgmbh--Llama-3-8b-instruct_2bitgs64_hqq/snapshots/ddd3622e909f63676b6e82d7d05ca6bb60c2c519\", max_model_len=4096)\n",
        "\n",
        "# Define sampling parameters\n",
        "sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=10)\n",
        "\n",
        "# Define a prompt for the model to generate text\n",
        "prompt = \"Once upon a time\"\n",
        "\n",
        "# Generate text using the model\n",
        "outputs = llm.generate(prompt, sampling_params=sampling_params)\n",
        "\n",
        "# Print the generated text\n",
        "for output in outputs:\n",
        "    print(output)\n"
      ],
      "metadata": {
        "id": "ddcH-eIGQT8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVizn-n8QoBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import bitblas\n",
        "print(bitblas.__file__)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ev-8jjRWQoXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show bitblas"
      ],
      "metadata": {
        "id": "CwYsRD2VQ5h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "en919B9iZmmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwHFNuT7ZmkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GMDC3mj5ZmiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lIo11nSRZmeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "patch_linearlayers(model, patch_add_quant_config,\n",
        "                          BaseQuantizeConfig(nbits=2, group_size=64, quant_scale=False, quant_zero=False, axis=1))\n",
        "\n",
        "model.eval();\n",
        "cleanup()\n",
        "\n",
        "#Use optimized inference kernels\n",
        "###################################################\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "#prepare_for_inference(model) #default backend\n",
        "#prepare_for_inference(model, backend=\"bitblas\", allow_merge=False) #It takes a while...\n",
        "\n",
        "#Generate\n",
        "###################################################\n",
        "#For longer context, make sure to allocate enough cache via the cache_size= parameter\n",
        "#gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=None) #Slower generation but no warm-up\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=20, do_sample=True, compile=\"partial\") #Faster generation, but warm-up takes a while\n",
        "\n",
        "\n",
        "gen.generate(\"How to make a yummy chocolate cake?\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "eAQdnniuZmbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRsnYaeiaTxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp_param = [p for p in model.parameters() if p.is_floating_point()]\n",
        "print(f\"عدد المعلمات العائمة: {len(fp_param)}\")\n",
        "print(f\"أنواع البيانات: {[p.dtype for p in fp_param]}\")\n",
        "#إذا كان العدد 0، فهذا يعني أن جميع المعلمات قد تم تحويلها إلى تنسيق كمي بالكامل.\n",
        "\n"
      ],
      "metadata": {
        "id": "OJ8SN8KlaT0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True)\n",
        "output = gen.generate(\"ما هو الذكاء الاصطناعي؟\", print_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "hauqgmzyahpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "0vJMFNRnbXo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "patch_linearlayers(model, patch_add_quant_config,\n",
        "                          BaseQuantizeConfig(nbits=2, group_size=64, quant_scale=False, quant_zero=False, axis=1))\n",
        "\n",
        "model.eval();\n",
        "cleanup()\n",
        "\n",
        "#Use optimized inference kernels\n",
        "###################################################\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "#prepare_for_inference(model) #default backend\n",
        "#prepare_for_inference(model, backend=\"bitblas\", allow_merge=False) #It takes a while...\n",
        "\n",
        "#Generate\n",
        "###################################################\n",
        "#For longer context, make sure to allocate enough cache via the cache_size= parameter\n",
        "#gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=None) #Slower generation but no warm-up\n",
        "#gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "#gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "#gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n",
        "#gen.generate(\"How to make a yummy chocolate cake?\", print_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True)\n",
        "output = gen.generate(\"What is artificial intelligence?\", print_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "xOBPeohxaoV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "patch_linearlayers(model, patch_add_quant_config,\n",
        "                          BaseQuantizeConfig(nbits=2, group_size=64, quant_scale=False, quant_zero=False, axis=1))\n",
        "\n",
        "model.eval();\n",
        "cleanup()\n",
        "\n",
        "#Use optimized inference kernels\n",
        "###################################################\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "#prepare_for_inference(model) #default backend\n",
        "#prepare_for_inference(model, backend=\"bitblas\", allow_merge=False) #It takes a while...\n",
        "\n",
        "#Generate\n",
        "###################################################\n",
        "#For longer context, make sure to allocate enough cache via the cache_size= parameter\n",
        "#gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=None) #Slower generation but no warm-up\n",
        "#gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "#gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "#gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n",
        "#gen.generate(\"How to make a yummy chocolate cake?\", print_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=30, do_sample=True)\n",
        "output = gen.generate(\"What is artificial intelligence?\", print_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "VkuAon5Wbagv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtype_ = patch_param.get(\"dtype\", torch.float32)  # اجعل torch.float32 القيمة الافتراضية\n",
        "3️⃣ تعديل patching.py لتجنب المشكلة\n",
        "إذا كنت بحاجة إلى prepare_for_inference()، يمكنك تعديل السطر الذي يسبب الخطأ في hqq/utils/patching.py:"
      ],
      "metadata": {
        "id": "09swue4yas0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtype_ = fp_param[0].dtype if (len(fp_param) > 0) else patch_param.get(\"dtype\", torch.float32)\n"
      ],
      "metadata": {
        "id": "flh_Ba7qcXgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not fp_param:\n",
        "    raise ValueError(\"fp_param is empty, meaning no floating-point parameters were found.\")\n"
      ],
      "metadata": {
        "id": "BhG7SjHocb9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"patch_param: {patch_param}\")\n",
        "print(f\"patch_param keys: {patch_param.keys() if patch_param else 'None'}\")\n"
      ],
      "metadata": {
        "id": "kmUL4Vaicgrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtype_ = fp_param[0].dtype if (len(fp_param) > 0) else patch_param[\"dtype\"]\n"
      ],
      "metadata": {
        "id": "4IQHgVcFckkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HQQModelForCausalLM.from_quantized(model_id)\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "uwtHu5b-cnoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "IdrZZkYxcsIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HQQModelForCausalLM.from_pretrained(model_id)\n"
      ],
      "metadata": {
        "id": "7JCpDiSKcvOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.generate(input_ids)\n",
        "print(\"Generated result:\", result)\n"
      ],
      "metadata": {
        "id": "Ua4CDXdScxpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "الخطوات لتشخيص وحل المشكلة:\n",
        "تفعيل CUDA_LAUNCH_BLOCKING: لتحديد مكان الخطأ بدقة أكبر، يمكنك تفعيل المتغير البيئي CUDA_LAUNCH_BLOCKING=1. هذا سيجبر CUDA على تنفيذ العمليات بشكل متزامن ويظهر لك موقع الخطأ الفعلي."
      ],
      "metadata": {
        "id": "oVmRgXFVdCEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
      ],
      "metadata": {
        "id": "UYJjNomOc066"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input tensor shape: {input_ids.shape}\")\n"
      ],
      "metadata": {
        "id": "PKnLozCCdEsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.set_debug_mode(True)\n"
      ],
      "metadata": {
        "id": "N8-DJlgzdE9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # لتفعيل تتبع الأخطاء\n",
        "\n",
        "# بعد تفعيل المتغير البيئي، حاول تشغيل الكود مرة أخرى\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=22, do_sample=True, compile=None)\n",
        "output = gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "ngvv87AXdJSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "# تحميل النموذج والمُحوّل\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# إعداد المتغيرات الخاصة بـ input_ids\n",
        "conversation = [{\"role\": \"user\", \"content\": \"What is artificial intelligence?\"}]\n",
        "input_ids = tokenizer.encode(conversation[0]['content'], return_tensors=\"pt\").to(device)\n",
        "attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)\n",
        "\n",
        "# استخدم `HFGenerator` للتوليد\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=30, do_sample=True)\n",
        "output = gen.generate(\"What is artificial intelligence?\", print_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "vgaSxtONdWai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "\n",
        "# تحديد الجهاز (CPU أو GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# تحميل النموذج والمُحوّل\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# إرسال النموذج إلى الجهاز المحدد (CPU أو GPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# إعداد المتغيرات الخاصة بـ input_ids\n",
        "conversation = [{\"role\": \"user\", \"content\": \"What is artificial intelligence?\"}]\n",
        "input_ids = tokenizer.encode(conversation[0]['content'], return_tensors=\"pt\").to(device)\n",
        "attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)\n",
        "\n",
        "# استخدم `HFGenerator` للتوليد\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=30, do_sample=True)\n",
        "output = gen.generate(\"What is artificial intelligence?\", print_tokens=True)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "AjaZMFX6duPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "ky7bv-XLet2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "patch_linearlayers(model, patch_add_quant_config,\n",
        "                          BaseQuantizeConfig(nbits=2, group_size=64, quant_scale=False, quant_zero=False, axis=1))\n",
        "\n",
        "model.eval();\n",
        "cleanup()\n",
        "\n",
        "#Use optimized inference kernels\n",
        "###################################################\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "#prepare_for_inference(model) #default backend\n",
        "#prepare_for_inference(model, backend=\"bitblas\", allow_merge=False) #It takes a while...\n",
        "\n",
        "#Generate\n",
        "###################################################\n",
        "#For longer context, make sure to allocate enough cache via the cache_size= parameter\n",
        "#gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=None) #Slower generation but no warm-up\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=1, do_sample=True, compile=\"partial\") #Faster generation, but warm-up takes a while\n",
        "\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n",
        "gen.generate(\"How to make a yummy chocolate cake?\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "xGJNY7MceZVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=1, do_sample=True, compile=\"partial\") #Faster generation, but warm-up takes a while\n",
        "\n",
        "gen.generate(\"who is ai?\", print_tokens=True)"
      ],
      "metadata": {
        "id": "EAfG5Rjzezv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=1000, do_sample=True, compile=\"partial\") #Faster generation, but warm-up takes a while\n",
        "\n",
        "gen.generate(\"who is ai?\", print_tokens=True)"
      ],
      "metadata": {
        "id": "JGvIK1jXe5zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=33, do_sample=True, compile=None)  # أو compile=\"none\"\n"
      ],
      "metadata": {
        "id": "zxXejI77fj_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen.generate(\"who is ai?\", print_tokens=True)"
      ],
      "metadata": {
        "id": "jZtfl-i7foZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=200, do_sample=True, temperature=0.1, top_k=50)\n"
      ],
      "metadata": {
        "id": "bMapnS33fpEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen.generate(\"Write a detailed essay about large language models, their uses in AI, and future developments.\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "YiJiYC-Ef-zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=20, do_sample=True, temperature=0.1, top_k=50, repetition_penalty=1.2)\n"
      ],
      "metadata": {
        "id": "g2G-v9i8gYAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=150, do_sample=True, temperature=0.6, top_k=50)\n"
      ],
      "metadata": {
        "id": "bsZDgXr6gmB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen.generate(\"Write a detailed essay about large language models, their uses in AI, and future developments.\", print_tokens=True)"
      ],
      "metadata": {
        "id": "Cc2emVt2gv62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=150, do_sample=True, temperature=0.6, top_k=50)\n",
        "\n",
        "# استخدام التوليد مع repetition_penalty\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "DCx7GxlWgwUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=150, do_sample=True, temperature=0.7, top_k=50)\n",
        "\n",
        "# Generate a more focused response\n",
        "gen.generate(\"Write an insightful essay about the future of AI and its impact on society\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "neuyyrPRg-1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=150, do_sample=True, temperature=0.7, top_k=50, top_p=0.9)\n",
        "gen.generate(\"Write a detailed essay on the impact of technology on society\", print_tokens=True, repetition_penalty=1.2)\n"
      ],
      "metadata": {
        "id": "Yl-m1m9HhPYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=150, do_sample=True)\n",
        "gen.generate(\"Write a detailed essay on the impact of technology on society\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "0emSa1-uhq2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen.generate(\n",
        "    \"Write a detailed essay on the impact of technology on society\",\n",
        "    print_tokens=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1.2,\n",
        "    max_new_tokens=150\n",
        ")\n"
      ],
      "metadata": {
        "id": "mwEQqzxvijMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=150, do_sample=True)\n",
        "gen.generate(\"Write a detailed essay on the impact of technology on society\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "K_F40lWXhvmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = chat_processor(\"How do I build a car?\", max_new_tokens=1000, do_sample=False)"
      ],
      "metadata": {
        "id": "L_zCReKQiDf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xW8WUXRIio4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bBwU0Smcio2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VorXPdlniozG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "15NFzut7kCmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#################################################################"
      ],
      "metadata": {
        "id": "yMDQO08NjTMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "SIIWQnsDiov1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "#prepare_for_inference(model, backend=\"torchao_int4\", verbose=True)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH_COMPILE)\n",
        "\n",
        "#Warmup\n",
        "for i in range(10):\n",
        "    with torch.no_grad():\n",
        "        out = model(torch.ones((1, 1), dtype=torch.int32, device='cuda'))\n",
        "del out\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "6BE3B7Z8iWG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from threading import Thread\n",
        "\n",
        "def chat_processor(chat, max_new_tokens=1, do_sample=True):\n",
        "    tokenizer.use_default_system_prompt = False\n",
        "    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generate_params = dict(\n",
        "        tokenizer(\"<s> [INST] \" + chat + \" [/INST] \", return_tensors=\"pt\").to('cuda'),\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        top_p=0.90,\n",
        "        top_k=50,\n",
        "        temperature= 0.6,\n",
        "        num_beams=1,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "\n",
        "    t = Thread(target=model.generate, kwargs=generate_params)\n",
        "    t.start()\n",
        "\n",
        "    print('------------------------------------------------------------')\n",
        "    cleanup()\n",
        "    print(chat); print();\n",
        "    outputs = []\n",
        "    for text in streamer:\n",
        "        outputs.append(text)\n",
        "        print(text, end=\"\", flush=True)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "################################################################################################\n",
        "#Generation\n",
        "outputs = chat_processor(\"How do I build a car?\", max_new_tokens=1000, do_sample=False)"
      ],
      "metadata": {
        "id": "o6UOrkzDiJPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###############################################################"
      ],
      "metadata": {
        "id": "tOU9y9gkjVJD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEzpbV-3kFdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYIkzbr5kFa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fhpq6FokFXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZdbeagckFUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLjg3B3mkFRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### شغال"
      ],
      "metadata": {
        "id": "D1dVjEeFlAwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####################################################################"
      ],
      "metadata": {
        "id": "Zy7Hks_-k_he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "nHjC1xsGkF4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "#prepare_for_inference(model, backend=\"torchao_int4\", verbose=True)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH_COMPILE)\n",
        "\n",
        "#Warmup\n",
        "for i in range(10):\n",
        "    with torch.no_grad():\n",
        "        out = model(torch.ones((1, 1), dtype=torch.int32, device='cuda'))\n",
        "del out\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "q7tbbQtrkF4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from threading import Thread\n",
        "\n",
        "def chat_processor(chat, max_new_tokens=100, do_sample=True):\n",
        "    tokenizer.use_default_system_prompt = False\n",
        "    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generate_params = dict(\n",
        "        tokenizer(\"<s> [INST] \" + chat + \" [/INST] \", return_tensors=\"pt\").to('cuda'),\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        top_p=0.80,\n",
        "        top_k=40,\n",
        "        temperature= 0.2,\n",
        "        num_beams=1,\n",
        "        repetition_penalty=1.3,\n",
        "    )\n",
        "\n",
        "    t = Thread(target=model.generate, kwargs=generate_params)\n",
        "    t.start()\n",
        "\n",
        "    print('------------------------------------------------------------')\n",
        "    cleanup()\n",
        "    print(chat); print();\n",
        "    outputs = []\n",
        "    for text in streamer:\n",
        "        outputs.append(text)\n",
        "        print(text, end=\"\", flush=True)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "################################################################################################\n",
        "#Generation\n",
        "outputs = chat_processor(\"How do I build a car?\", max_new_tokens=100, do_sample=True)"
      ],
      "metadata": {
        "id": "33x3Eab0kF4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###################################################"
      ],
      "metadata": {
        "id": "mmO9ZuDdk77Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "87KoGSElkphy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}