{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xnPXqFIpY_x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDu93CVNqBVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hqq==0.1.8\n",
        "!pip install transformers==4.46.0"
      ],
      "metadata": {
        "id": "w6li0zQLqBYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "K9ZxfDd1qBat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "\n",
        "#Pytorch backend that makes the model compatible with fullgrah torch.compile: works with any settings\n",
        "#prepare_for_inference(model)\n"
      ],
      "metadata": {
        "id": "eKaLvedeqnQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH)\n",
        "#prepare_for_inference(model)\n",
        "prepare_for_inference(model, backend=\"torchao_int4\")\n"
      ],
      "metadata": {
        "id": "FWPVRKGTqw1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=100, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n",
        "gen.generate(\"How to make a yummy chocolate cake?\", print_tokens=True)"
      ],
      "metadata": {
        "id": "vjC5xzbuqmmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-zaJxrpvFrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uo1jlYyWvFom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WtCWKmMAvFll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cTgoLYQvFi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lpJG5VHUvFfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zKo-Xk43vFcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fis29eR5vFZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E8GpwuCmvFWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitblas"
      ],
      "metadata": {
        "id": "7EQKUbhVvexe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "jHFQjKxW5lSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "n-EIBQrbvFTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HQQLinear.set_backend(HQQBackend.PYTORCH)"
      ],
      "metadata": {
        "id": "v6VP5c47vzYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "\n",
        "#Pytorch backend that makes the model compatible with fullgrah torch.compile: works with any settings\n",
        "#prepare_for_inference(model)\n",
        "\n",
        "#Torchao's tiny_gemm backned (fastest): nbits=4, compute_dtype=bfloat16, axis=1\n",
        "#prepare_for_inference(model, backend=\"torchao_int4\")\n",
        "\n",
        "#Gemlite backend: nbits=4/2/1, compute_dtype=float16, axis=1\n",
        "#prepare_for_inference(model, backend=\"gemlite\")\n",
        "\n",
        "#Bitblas backend: nbits=4/2/1, compute_dtype=float16, axis=1\n",
        "prepare_for_inference(model, backend=\"bitblas\")"
      ],
      "metadata": {
        "id": "UXH3tEq2vFP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=10, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "\n",
        "gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "9Ro0yKjVvFMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TZb6ygMzhJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4V40NFeazhGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "an9z2QpzzhDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4_N1994zg_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRrGs2DEzg5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-43RcEXzg2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "5GpczI8x5il5"
      }
    },
    {
      "source": [
        "from hqq.utils.patching import prepare_for_inference, patch_linearlayers, patch_add_weight_param\n",
        "import torch\n",
        "\n",
        "def prepare_for_inference(model, allow_merge=False, backend=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Patch the model to prepare for inference.\n",
        "    \"\"\"\n",
        "    # ...other code...\n",
        "\n",
        "    def patch_add_weight_param(layer, patch_param):\n",
        "        \"\"\"\n",
        "        Patch a linear layer to add a weight parameter.\n",
        "        \"\"\"\n",
        "        # ...other code...\n",
        "        fp_param = [p for p in layer.parameters() if p.is_floating_point()]\n",
        "\n",
        "        # Check if fp_param is empty and provide a default dtype if necessary\n",
        "        dtype_ = fp_param[0].dtype if fp_param else patch_param[\"dtype\"] if patch_param and \"dtype\" in patch_param else torch.float32\n",
        "\n",
        "        # ...rest of the code...\n",
        "\n",
        "    patch_linearlayers(\n",
        "        model, patch_add_weight_param, {\"device\": model.device, \"dtype\": model.dtype}\n",
        "    )\n",
        "\n",
        "# ...rest of your code..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "q35zRnEk0uNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "rl9kcl4-5hoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "M_Mzt6zCzhtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "import torch\n",
        "prepare_for_inference(model, allow_merge=False, backend=None, verbose=False)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH)"
      ],
      "metadata": {
        "id": "VgDvrTgs1SI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "prepare_for_inference(model, backend=\"\", verbose=True)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH_COMPILE)\n"
      ],
      "metadata": {
        "id": "OCbiQ0xY1k7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "F_Anur7G5ftC"
      }
    },
    {
      "source": [
        "from hqq.utils.patching import prepare_for_inference, patch_linearlayers, patch_add_weight_param\n",
        "import torch\n",
        "\n",
        "def prepare_for_inference(model, allow_merge=False, backend=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Patch the model to prepare for inference.\n",
        "    \"\"\"\n",
        "    # ...other code...\n",
        "\n",
        "    def patch_add_weight_param(layer, patch_param):\n",
        "        \"\"\"\n",
        "        Patch a linear layer to add a weight parameter.\n",
        "        \"\"\"\n",
        "        # ...other code...\n",
        "        fp_param = [p for p in layer.parameters() if p.is_floating_point()]\n",
        "\n",
        "        # Check if fp_param is empty and provide a default dtype if necessary\n",
        "        # If fp_param is empty, use patch_param[\"dtype\"] if available, otherwise use torch.float32\n",
        "        dtype_ = fp_param[0].dtype if fp_param else patch_param.get(\"dtype\", torch.float32)\n",
        "\n",
        "        # ...rest of the code...\n",
        "\n",
        "    patch_linearlayers(\n",
        "        model, patch_add_weight_param, {\"device\": model.device, \"dtype\": model.dtype}\n",
        "    )\n",
        "\n",
        "# ...rest of your code..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NCnuPsP12F7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference, patch_linearlayers, patch_add_weight_param\n",
        "import torch\n",
        "\n",
        "def prepare_for_inference(model, allow_merge=False, backend=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Patch the model to prepare for inference.\n",
        "    \"\"\"\n",
        "    # ...other code...\n",
        "\n",
        "    def patch_add_weight_param(layer, patch_param):\n",
        "        \"\"\"\n",
        "        Patch a linear layer to add a weight parameter.\n",
        "        \"\"\"\n",
        "        # ...other code...\n",
        "        fp_param = [p for p in layer.parameters() if p.is_floating_point()]\n",
        "\n",
        "        # Check if fp_param is empty and provide a default dtype if necessary\n",
        "        dtype_ = fp_param[0].dtype if fp_param else patch_param[\"dtype\"] if patch_param and \"dtype\" in patch_param else torch.float32\n",
        "\n",
        "        # ...rest of the code...\n",
        "\n",
        "    patch_linearlayers(\n",
        "        model, patch_add_weight_param, {\"device\": model.device, \"dtype\": model.dtype}\n",
        "    )\n",
        "\n",
        "# ...rest of your code...\n",
        "\n",
        "\n",
        "\n",
        "from hqq.utils.patching import prepare_for_inference\n",
        "import torch\n",
        "prepare_for_inference(model)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH)"
      ],
      "metadata": {
        "id": "e3Jg_R0MzhtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "rvVtWiO_5ecZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=1, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "\n",
        "gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "a0ZtZvqXzhtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9uuisBlI2Uqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZvhLFc42Un3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "Vno86Hj05dEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=5, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "\n",
        "gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "jgzZTU_X2UlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.attention.sdpa_kernel()"
      ],
      "metadata": {
        "id": "DqgICFiZ2Y2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import scaled_dot_product_attention\n",
        "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
        "# Only enable flash attention backend\n",
        "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
        "    scaled_dot_product_attention(...)\n",
        "\n",
        "# Enable the Math or Efficient attention backends\n",
        "with sdpa_kernel([SDPBackend.MATH, SDPBackend.EFFICIENT_ATTENTION]):\n",
        "    scaled_dot_product_attention(...)"
      ],
      "metadata": {
        "id": "JfKrsDd64pjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"\n",
        "    Compute the scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        query: Query tensor of shape (batch_size, num_heads, seq_len, d_k)\n",
        "        key: Key tensor of shape (batch_size, num_heads, seq_len, d_k)\n",
        "        value: Value tensor of shape (batch_size, num_heads, seq_len, d_v)\n",
        "        mask: Optional mask tensor of shape (batch_size, 1, 1, seq_len)\n",
        "\n",
        "    Returns:\n",
        "        attention_output: Output tensor of shape (batch_size, num_heads, seq_len, d_v)\n",
        "        attention_weights: Attention weights tensor of shape (batch_size, num_heads, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    attention_output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return attention_output, attention_weights\n",
        "\n",
        "# Example usage\n",
        "batch_size = 2\n",
        "num_heads = 4\n",
        "seq_len = 5\n",
        "d_k = d_v = 64\n",
        "\n",
        "query = torch.rand(batch_size, num_heads, seq_len, d_k)\n",
        "key = torch.rand(batch_size, num_heads, seq_len, d_k)\n",
        "value = torch.rand(batch_size, num_heads, seq_len, d_v)\n",
        "mask = torch.ones(batch_size, 1, 1, seq_len)  # Optional mask\n",
        "\n",
        "attention_output, attention_weights = scaled_dot_product_attention(query, key, value, mask)\n",
        "print(\"Attention Output:\", attention_output)\n",
        "print(\"Attention Weights:\", attention_weights)\n"
      ],
      "metadata": {
        "id": "kv8QqVtg2YzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1YFg_8N2Ytw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5erP9on2Yqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OtyOdeCQ2rn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dR46R0TO2rly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovegc9HK2ri8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cco_LnYP2rfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D9PVUpke2rcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install hqq==0.1.8\n",
        "!pip install transformers==4.46.0\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "# Load the model\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Use the default PyTorch backend\n",
        "from hqq.utils.patching import prepare_for_inference\n",
        "prepare_for_inference(model)\n",
        "\n",
        "# Initialize the generator\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=100, do_sample=True, compile=\"partial\").warmup()\n",
        "\n",
        "# Generate text\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n",
        "gen.generate(\"How to make a yummy chocolate cake?\", print_tokens=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bFXFAtzh2sz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install hqq==0.1.8\n",
        "!pip install transformers==4.46.0\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "# Load the model\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Enable torchao_int4 backend for faster inference\n",
        "from hqq.utils.patching import prepare_for_inference\n",
        "prepare_for_inference(model, backend=\"torchao_int4\")  # Using torchao_int4 backend\n",
        "\n",
        "# Initialize the generator\n",
        "gen = HFGenerator(model, tokenizer, max_new_tokens=100, do_sample=True, compile=\"partial\").warmup()\n",
        "\n",
        "# Generate text\n",
        "gen.generate(\"Write an essay about large language models\", print_tokens=True)\n",
        "gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n",
        "gen.generate(\"How to make a yummy chocolate cake?\", print_tokens=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BnuE1H8U2Zvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JhUdOSx-5BQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NScVF1Qm5BK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDPdI6jb5BH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZS6FIMh-5BE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jvf1EZ_5BBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchao"
      ],
      "metadata": {
        "id": "68MjrhLp7FmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "prepare_for_inference(model, backend=\"torchao_int4\", verbose=True)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH_COMPILE)"
      ],
      "metadata": {
        "id": "CW7b4Ylu7Ksq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "prepare_for_inference(model, backend=\"pytorch\", verbose=True)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH_COMPILE)"
      ],
      "metadata": {
        "id": "AQ_iWSLe7T0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchao\n",
        "\n",
        "from hqq.utils.patching import prepare_for_inference\n",
        "prepare_for_inference(model, backend=\"torchao_int4\", verbose=True)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH_COMPILE)\n"
      ],
      "metadata": {
        "id": "IWLS4hsE8JlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "\n",
        "prepare_for_inference(model)  # Using the default PyTorch backend"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "q38ceBoM6GGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "1EO8Ewok84Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from hqq.utils.patching import prepare_for_inference\n",
        "prepare_for_inference(model, backend=\"pytorch\", verbose=True)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH_COMPILE)"
      ],
      "metadata": {
        "id": "svqtZiZ55A-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPQdnU9f5A6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from hqq.utils.patching import prepare_for_inference\n",
        "prepare_for_inference(model, backend=\"torchao_int4\", verbose=True)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH_COMPILE)\n",
        "\n",
        "#Warmup\n",
        "for i in range(10):\n",
        "    with torch.no_grad():\n",
        "        out = model(torch.ones((1, 1), dtype=torch.int32, device='cuda'))\n",
        "del out\n",
        "cleanup()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "import transformers\n",
        "from threading import Thread\n",
        "\n",
        "def chat_processor(chat, max_new_tokens=100, do_sample=True):\n",
        "    tokenizer.use_default_system_prompt = False\n",
        "    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generate_params = dict(\n",
        "        tokenizer(\"<s> [INST] \" + chat + \" [/INST] \", return_tensors=\"pt\").to('cuda'),\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        top_p=0.90,\n",
        "        top_k=50,\n",
        "        temperature= 0.6,\n",
        "        num_beams=1,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "\n",
        "    t = Thread(target=model.generate, kwargs=generate_params)\n",
        "    t.start()\n",
        "\n",
        "    print('------------------------------------------------------------')\n",
        "    cleanup()\n",
        "    print(chat); print();\n",
        "    outputs = []\n",
        "    for text in streamer:\n",
        "        outputs.append(text)\n",
        "        print(text, end=\"\", flush=True)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "################################################################################################\n",
        "#Generation\n",
        "outputs = chat_processor(\"How do I build a car?\", max_new_tokens=1000, do_sample=False)"
      ],
      "metadata": {
        "id": "1BV3yyQP7pxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=1, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "\n",
        "gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "Q3-NP3DA7p0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_for_inference(model, backend=NoneType, verbose=True)"
      ],
      "metadata": {
        "id": "PRf7DSFB7p4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from hqq.utils.patching import prepare_for_inference\n",
        "\n",
        "prepare_for_inference(model, verbose=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "CH3UErzs-Jp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "model_id   = \"mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq\"\n",
        "cache_path = '.'\n",
        "compute_dtype = torch.bfloat16\n",
        "model      = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_path, torch_dtype=compute_dtype, attn_implementation=\"flash_attention_2\")\n",
        "tokenizer  = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_path)\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "#Set backend\n",
        "from hqq.utils.patching import prepare_for_inference\n",
        "prepare_for_inference(model, backend=\"torchao_int4\", verbose=True)\n",
        "\n",
        "HQQLinear.set_backend(HQQBackend.PYTORCH_COMPILE)\n",
        "\n",
        "#Warmup\n",
        "for i in range(10):\n",
        "    with torch.no_grad():\n",
        "        out = model(torch.ones((1, 1), dtype=torch.int32, device='cuda'))\n",
        "del out\n",
        "cleanup()\n",
        "\n",
        "################################################################################################\n",
        "import transformers\n",
        "from threading import Thread\n",
        "\n",
        "def chat_processor(chat, max_new_tokens=100, do_sample=True):\n",
        "    tokenizer.use_default_system_prompt = False\n",
        "    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generate_params = dict(\n",
        "        tokenizer(\"<s> [INST] \" + chat + \" [/INST] \", return_tensors=\"pt\").to('cuda'),\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        top_p=0.90,\n",
        "        top_k=50,\n",
        "        temperature= 0.6,\n",
        "        num_beams=1,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "\n",
        "    t = Thread(target=model.generate, kwargs=generate_params)\n",
        "    t.start()\n",
        "\n",
        "    print('------------------------------------------------------------')\n",
        "    cleanup()\n",
        "    print(chat); print();\n",
        "    outputs = []\n",
        "    for text in streamer:\n",
        "        outputs.append(text)\n",
        "        print(text, end=\"\", flush=True)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "################################################################################################\n",
        "#Generation\n",
        "outputs = chat_processor(\"How do I build a car?\", max_new_tokens=1000, do_sample=False)"
      ],
      "metadata": {
        "id": "Q67UO6nz7p6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBVmMxUv7p-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cdYjelmA7qBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_eoPHOE17qEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkXB-Idn7qHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from hqq.models.hf.base import AutoHQQHFModel\n",
        "from hqq.core.quantize import *\n",
        "from hqq.utils.patching import *\n",
        "from hqq.utils.generation_hf import HFGenerator\n",
        "\n",
        "#Load the model\n",
        "###################################################\n",
        "model_id = 'mobiuslabsgmbh/Llama-3-8b-instruct_2bitgs64_hqq'\n",
        "model     = AutoHQQHFModel.from_quantized(model_id, cache_dir='.', compute_dtype=torch.float16, adapter='adapter_v0.1.lora')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "R2g9UHi85B4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from hqq.utils.patching import prepare_for_inference, patch_linearlayers, patch_add_weight_param\n",
        "import torch\n",
        "\n",
        "def prepare_for_inference(model, allow_merge=False, backend=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Patch the model to prepare for inference.\n",
        "    \"\"\"\n",
        "    # ...other code...\n",
        "\n",
        "    def patch_add_weight_param(layer, patch_param):\n",
        "        \"\"\"\n",
        "        Patch a linear layer to add a weight parameter.\n",
        "        \"\"\"\n",
        "        # ...other code...\n",
        "        fp_param = [p for p in layer.parameters() if p.is_floating_point()]\n",
        "\n",
        "        # Check if fp_param is empty and provide a default dtype if necessary\n",
        "        # If fp_param is empty, use patch_param[\"dtype\"] if available, otherwise use torch.float32\n",
        "        dtype_ = fp_param[0].dtype if fp_param else patch_param.get(\"dtype\", torch.float32)\n",
        "\n",
        "        # ...rest of the code...\n",
        "\n",
        "    patch_linearlayers(\n",
        "        model, patch_add_weight_param, {\"device\": model.device, \"dtype\": model.dtype}\n",
        "    )\n",
        "\n",
        "# ...rest of your code..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "w6mFuvR65B4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = HFGenerator(model, tokenizer, max_new_tokens=1, do_sample=True, compile=\"partial\").warmup() #Faster generation, but warm-up takes a while\n",
        "\n",
        "\n",
        "gen.generate(\"Tell me a funny joke!\", print_tokens=True)\n"
      ],
      "metadata": {
        "id": "gfAUh2Lr5B4U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}