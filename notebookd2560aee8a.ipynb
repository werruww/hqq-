{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-06T22:55:12.613503Z","iopub.execute_input":"2025-02-06T22:55:12.613743Z","iopub.status.idle":"2025-02-06T22:55:13.014811Z","shell.execute_reply.started":"2025-02-06T22:55:12.613719Z","shell.execute_reply":"2025-02-06T22:55:13.013868Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install hqq==0.1.8\n!pip install transformers==4.46.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T22:57:50.014841Z","iopub.execute_input":"2025-02-06T22:57:50.015128Z","iopub.status.idle":"2025-02-06T22:59:55.131719Z","shell.execute_reply.started":"2025-02-06T22:57:50.015104Z","shell.execute_reply":"2025-02-06T22:59:55.130920Z"}},"outputs":[{"name":"stdout","text":"Collecting hqq==0.1.8\n  Downloading hqq-0.1.8.tar.gz (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.8) (1.26.4)\nRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.8) (4.67.1)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.8) (0.8.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.8) (1.2.1)\nRequirement already satisfied: transformers>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.8) (4.47.0)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.8) (0.27.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from hqq==0.1.8) (2.5.0)\nCollecting bitblas (from hqq==0.1.8)\n  Downloading bitblas-0.1.0-py3-none-manylinux1_x86_64.whl.metadata (19 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->hqq==0.1.8) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->hqq==0.1.8) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->hqq==0.1.8) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->hqq==0.1.8) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->hqq==0.1.8) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->hqq==0.1.8) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.8) (3.16.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.8) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.8) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.8) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.8) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.8) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.36.1->hqq==0.1.8) (0.4.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->hqq==0.1.8) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->hqq==0.1.8) (4.12.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->hqq==0.1.8) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->hqq==0.1.8) (2.5.1+cu121)\nRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (1.17.1)\nCollecting cpplint (from bitblas->hqq==0.1.8)\n  Downloading cpplint-2.0.0-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (3.0.11)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (4.4.2)\nRequirement already satisfied: docutils in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (0.21.2)\nCollecting dtlib (from bitblas->hqq==0.1.8)\n  Downloading dtlib-0.0.0.dev2-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pytest>=6.2.4 in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (8.3.4)\nCollecting pytest-xdist>=2.2.1 (from bitblas->hqq==0.1.8)\n  Downloading pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (24.3.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (3.1.0)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (0.4.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (1.13.1)\nRequirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (6.3.3)\nCollecting thefuzz (from bitblas->hqq==0.1.8)\n  Downloading thefuzz-0.22.1-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from bitblas->hqq==0.1.8) (0.9.0)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.4->bitblas->hqq==0.1.8) (1.2.2)\nRequirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.4->bitblas->hqq==0.1.8) (2.0.0)\nRequirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.4->bitblas->hqq==0.1.8) (1.5.0)\nRequirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.4->bitblas->hqq==0.1.8) (2.2.1)\nCollecting execnet>=2.1 (from pytest-xdist>=2.2.1->bitblas->hqq==0.1.8)\n  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->hqq==0.1.8) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->hqq==0.1.8) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate->hqq==0.1.8) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate->hqq==0.1.8) (1.3.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->bitblas->hqq==0.1.8) (2.22)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->hqq==0.1.8) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->hqq==0.1.8) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->hqq==0.1.8) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->hqq==0.1.8) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.1->hqq==0.1.8) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.1->hqq==0.1.8) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.1->hqq==0.1.8) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.36.1->hqq==0.1.8) (2024.12.14)\nCollecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz->bitblas->hqq==0.1.8)\n  Downloading rapidfuzz-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->hqq==0.1.8) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate->hqq==0.1.8) (3.0.2)\nDownloading bitblas-0.1.0-py3-none-manylinux1_x86_64.whl (88.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cpplint-2.0.0-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dtlib-0.0.0.dev2-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\nDownloading execnet-2.1.1-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: hqq\n  Building wheel for hqq (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for hqq: filename=hqq-0.1.8-py3-none-any.whl size=61842 sha256=2871c10d7687fca9e765a248b0ebf2d8f83870c65fb001aa084d584142f1de63\n  Stored in directory: /root/.cache/pip/wheels/2a/e1/f7/c4c3cd25448d1ce4f8f4c8ed26f05c890d8c6c06c16c5800da\nSuccessfully built hqq\nInstalling collected packages: cpplint, rapidfuzz, execnet, dtlib, thefuzz, pytest-xdist, bitblas, hqq\nSuccessfully installed bitblas-0.1.0 cpplint-2.0.0 dtlib-0.0.0.dev2 execnet-2.1.1 hqq-0.1.8 pytest-xdist-3.6.1 rapidfuzz-3.12.1 thefuzz-0.22.1\nCollecting transformers==4.46.0\n  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (0.4.5)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.46.0)\n  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.46.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.46.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.46.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.46.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.46.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers==4.46.0) (2024.2.0)\n\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.46.0 at https://files.pythonhosted.org/packages/db/88/1ef8a624a33d7fe460a686b9e0194a7916320fc0d67d4e38e570beeac039/transformers-4.46.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.8.0))\nReason for being yanked: This version unfortunately does not work with 3.8 but we did not drop the support yet\u001b[0m\u001b[33m\n\u001b[0mDownloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\nSuccessfully installed tokenizers-0.20.3 transformers-4.46.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers \nfrom threading import Thread\n\nmodel_id = 'mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-metaoffload-HQQ'\n#Load the model\nfrom hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel     = HQQModelForCausalLM.from_quantized(model_id)\n\n#Optional: set backend/compile\n#You will need to install CUDA kernels apriori\n# git clone https://github.com/mobiusml/hqq/\n# cd hqq/kernels && python setup_cuda.py install\nfrom hqq.core.quantize import *\nHQQLinear.set_backend(HQQBackend.ATEN_BACKPROP)\n\n\ndef chat_processor(chat, max_new_tokens=1, do_sample=True):\n    tokenizer.use_default_system_prompt = False\n    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n\n    generate_params = dict(\n        tokenizer(\"<s> [INST] \" + chat + \" [/INST] \", return_tensors=\"pt\").to('cuda'),\n        streamer=streamer,\n        max_new_tokens=max_new_tokens,\n        do_sample=do_sample,\n        top_p=0.90,\n        top_k=50,\n        temperature= 0.6,\n        num_beams=1,\n        repetition_penalty=1.2,\n    )\n\n    t = Thread(target=model.generate, kwargs=generate_params)\n    t.start()\n    outputs = []\n    for text in streamer:\n        outputs.append(text)\n        print(text, end=\"\", flush=True)\n\n    return outputs\n\n################################################################################################\n#Generation\noutputs = chat_processor(\"How do I build a car?\", max_new_tokens=1, do_sample=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T23:00:07.718819Z","iopub.execute_input":"2025-02-06T23:00:07.719208Z","iopub.status.idle":"2025-02-06T23:09:11.520250Z","shell.execute_reply.started":"2025-02-06T23:00:07.719177Z","shell.execute_reply":"2025-02-06T23:09:11.519368Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e491e0aea1648768af1e5dd253d55d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f6fe1d269d64f46bd1bcd0b40dec62d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22e1fcbab2fc451d9cb48428627a5918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b1eec3a7764ac49c14f33e426a367e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ab775d3659f4deab79c14b42367606d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539dd5577af748c283459e5fee9371f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b66b180c8b6d4b5887ac3e03874a9b3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99bd430e525444793abe7a7440ce2f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f7630cae6cd440392f288c147e3f6ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f11fc2df30064784a759908fdf66ce47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3580a27dcfe8474aa17c3d0fabb035ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90147d0ac6734ff5b53d2f0f08ae4006"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"qmodel.pt:   0%|          | 0.00/18.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae4e851133b14b0bb5fe1d701ac780a3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/hqq/models/base.py:237: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(cls.get_weight_file(save_dir), map_location=map_location)\n100%|██████████| 32/32 [00:00<00:00, 117.13it/s]\n100%|██████████| 32/32 [00:11<00:00,  2.78it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ATEN/CUDA backend not availabe. Make sure you install the hqq_aten library.\n1","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import transformers \nfrom threading import Thread\n\nmodel_id = 'mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-metaoffload-HQQ'\n#Load the model\nfrom hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel     = HQQModelForCausalLM.from_quantized(model_id)\n\n#Optional: set backend/compile\n#You will need to install CUDA kernels apriori\n# git clone https://github.com/mobiusml/hqq/\n# cd hqq/kernels && python setup_cuda.py install\nfrom hqq.core.quantize import *\nHQQLinear.set_backend(HQQBackend.ATEN_BACKPROP)\n\n\ndef chat_processor(chat, max_new_tokens=1, do_sample=True):\n    tokenizer.use_default_system_prompt = False\n    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n\n    generate_params = dict(\n        tokenizer(\"<s> [INST] \" + chat + \" [/INST] \", return_tensors=\"pt\").to('cuda'),\n        streamer=streamer,\n        max_new_tokens=max_new_tokens,\n        do_sample=do_sample,\n        top_p=0.90,\n        top_k=50,\n        temperature= 0.6,\n        num_beams=1,\n        repetition_penalty=1.2,\n    )\n\n    t = Thread(target=model.generate, kwargs=generate_params)\n    t.start()\n    outputs = []\n    for text in streamer:\n        outputs.append(text)\n        print(text, end=\"\", flush=True)\n\n    return outputs\n\n################################################################################################\n#Generation\noutputs = chat_processor(\"How do I build a car?\", max_new_tokens=1, do_sample=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outputs = chat_processor(\"How do I build a car?\", max_new_tokens=100, do_sample=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T23:10:13.748764Z","iopub.execute_input":"2025-02-06T23:10:13.749433Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1. Design the Car: The first step in building a car is to design it. This includes deciding on the type of car you want to build (e.g., sedan, ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import hqq_aten","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}